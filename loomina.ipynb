{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Installation in Jupyter Notebook\n",
    "\n",
    "##### Using `%pip` for Installation\n",
    "\n",
    "To install packages using `pip`, you can run the following commands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers trl==0.4.7\n",
    "# %pip install -q datasets==2.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==0.21.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from accelerate==0.21.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from accelerate==0.21.0) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from accelerate==0.21.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from accelerate==0.21.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from accelerate==0.21.0) (2.4.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.21.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.21.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate==0.21.0) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft==0.4.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from peft==0.4.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from peft==0.4.0) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from peft==0.4.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from peft==0.4.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from peft==0.4.0) (2.4.0+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from peft==0.4.0) (4.44.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from peft==0.4.0) (0.21.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from peft==0.4.0) (0.4.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.4.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.4.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.4.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.4.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from torch>=1.13.0->peft==0.4.0) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from transformers->peft==0.4.0) (0.24.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from transformers->peft==0.4.0) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from transformers->peft==0.4.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from transformers->peft==0.4.0) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from transformers->peft==0.4.0) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers->peft==0.4.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from requests->transformers->peft==0.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from requests->transformers->peft==0.4.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from requests->transformers->peft==0.4.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from requests->transformers->peft==0.4.0) (2024.7.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install peft==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bitsandbytes==0.40.2\n",
    "# %pip install trl==0.4.7\n",
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.19.2 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (2.19.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.1 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.2) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (3.10.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (0.24.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from datasets==2.19.2) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from aiohttp->datasets==2.19.2) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from aiohttp->datasets==2.19.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from aiohttp->datasets==2.19.2) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from aiohttp->datasets==2.19.2) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from aiohttp->datasets==2.19.2) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from aiohttp->datasets==2.19.2) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets==2.19.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from requests>=2.32.1->datasets==2.19.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from requests>=2.32.1->datasets==2.19.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from requests>=2.32.1->datasets==2.19.2) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from requests>=2.32.1->datasets==2.19.2) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.19.2) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from pandas->datasets==2.19.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from pandas->datasets==2.19.2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from pandas->datasets==2.19.2) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\documents\\loom\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.19.2) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets==2.19.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\Documents\\loom\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available:  True\n",
      "cuDNN version:  90100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available: \", torch.cuda.is_available())\n",
    "print(\"cuDNN version: \", torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning\n",
    "\n",
    "##### Setting the parameter for training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"shrujan142/LoomAI\"\n",
    "\n",
    "dataset_name = \"formatted_1000_stories\"\n",
    "\n",
    "new_model = \"Llama-2-7b-chat-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRa attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Computer dtype for 4-bti base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output directory where thr model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = True\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examlpes in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset (you can process it here)\n",
    "# def load_dataset(file_path, split=\"train\"):\n",
    "#     # Construct the file path based on the split\n",
    "#     split_file_path = file_path.replace(\"train\", split)\n",
    "\n",
    "#     # Read the text file\n",
    "#     with open(split_file_path, 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "\n",
    "#     # Create a Hugging Face Dataset\n",
    "#     data = {'text': [line.strip() for line in lines]}\n",
    "#     dataset = Dataset.from_dict(data)\n",
    "\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "# Detect encoding\n",
    "with open(\"formatted_1000_stories.txt\", 'rb') as file:\n",
    "    result = chardet.detect(file.read())\n",
    "encoding = result['encoding']\n",
    "\n",
    "# Load dataset with detected encoding\n",
    "def load_dataset(file_path, split, encoding):\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        lines = file.readlines()\n",
    "    data = {'text': [line.strip() for line in lines]}\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 26302 examples [00:00, 774801.65 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.69s/it]\n",
      "c:\\Users\\admin\\Documents\\loom\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\admin\\Documents\\loom\\.venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:289: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\Documents\\loom\\.venv\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map:   0%|          | 0/26302 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 26302/26302 [00:00<00:00, 59740.49 examples/s]\n",
      "c:\\Users\\admin\\Documents\\loom\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\Documents\\loom\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "  0%|          | 0/1644 [00:00<?, ?it/s]c:\\Users\\admin\\Documents\\loom\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  2%|▏         | 25/1644 [00:22<20:05,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1243, 'grad_norm': 0.12248802185058594, 'learning_rate': 0.0001, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 50/1644 [00:48<29:40,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2985.5966, 'grad_norm': 0.0, 'learning_rate': 0.0002, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 75/1644 [01:17<29:50,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0881, 'grad_norm': 0.1004718691110611, 'learning_rate': 0.00019987863728928486, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 100/1644 [01:44<27:34,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 724.115, 'grad_norm': 0.0, 'learning_rate': 0.0001995148437352905, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 125/1644 [02:13<29:24,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1445, 'grad_norm': 0.1694830060005188, 'learning_rate': 0.000198909502357454, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 150/1644 [02:36<17:13,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3242.6838, 'grad_norm': 0.0, 'learning_rate': 0.00019806408247318577, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 175/1644 [02:57<17:41,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4012, 'grad_norm': 0.22641393542289734, 'learning_rate': 0.00019698063613146277, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 200/1644 [03:17<26:10,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0001956617931319839, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 225/1644 [03:47<27:56,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.141, 'grad_norm': 0.15559852123260498, 'learning_rate': 0.00019411075464197773, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 250/1644 [04:16<27:07,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4626.2028, 'grad_norm': 0.0, 'learning_rate': 0.00019233128542615557, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 275/1644 [04:45<26:13,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1115, 'grad_norm': 0.1486511081457138, 'learning_rate': 0.0001903277047086708, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 300/1644 [05:14<25:55,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3512.8947, 'grad_norm': 0.0, 'learning_rate': 0.00018810487568926362, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 325/1644 [05:43<26:27,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1368, 'grad_norm': 0.21006520092487335, 'learning_rate': 0.00018566819373903908, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 350/1644 [06:11<23:19,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8807.5925, 'grad_norm': 0.0, 'learning_rate': 0.00018302357330452973, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 375/1644 [06:40<23:48,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0823, 'grad_norm': 0.15692530572414398, 'learning_rate': 0.0001801774335518305, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 400/1644 [07:08<23:06,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1959.2627, 'grad_norm': 0.0, 'learning_rate': 0.0001771366827856506, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 425/1644 [07:37<23:34,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1065, 'grad_norm': 0.16296902298927307, 'learning_rate': 0.00017390870168110194, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 450/1644 [08:05<22:39,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2527.8837, 'grad_norm': 0.0, 'learning_rate': 0.00017050132536892414, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 475/1644 [08:34<22:53,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.149, 'grad_norm': 0.1952982246875763, 'learning_rate': 0.00016692282441763072, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 500/1644 [08:55<13:01,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00016318188475873658, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 525/1644 [09:17<15:24,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0976, 'grad_norm': 0.14883507788181305, 'learning_rate': 0.0001592875866037942, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 550/1644 [09:44<20:25,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1677.3963, 'grad_norm': 0.0, 'learning_rate': 0.0001552493824044119, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 575/1644 [10:11<13:40,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1363, 'grad_norm': 0.25786828994750977, 'learning_rate': 0.0001510770739087509, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 600/1644 [10:28<11:45,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9038.0375, 'grad_norm': 0.0, 'learning_rate': 0.00014678078837019064, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 625/1644 [10:51<17:46,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1423, 'grad_norm': 0.212635800242424, 'learning_rate': 0.0001423709539659104, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 650/1644 [11:18<17:24,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6417.7425, 'grad_norm': 0.0, 'learning_rate': 0.00013785827448505242, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 675/1644 [11:46<18:25,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2448, 'grad_norm': 0.5004977583885193, 'learning_rate': 0.0001332537033479043, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 700/1644 [12:10<15:34,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00012856841701916388, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 725/1644 [12:31<11:15,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2412, 'grad_norm': 0.4630655348300934, 'learning_rate': 0.0001238137878798177, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 750/1644 [12:49<10:13,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.00011900135662348173, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 775/1644 [13:10<11:02,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0617, 'grad_norm': 0.13702687621116638, 'learning_rate': 0.00011414280424420394, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 800/1644 [13:27<09:37,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 617.485, 'grad_norm': 0.0, 'learning_rate': 0.00010924992368372236, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 825/1644 [13:53<11:10,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0869, 'grad_norm': 0.16260093450546265, 'learning_rate': 0.0001043345912069975, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 850/1644 [14:10<08:59,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9821.0181, 'grad_norm': 0.0, 'learning_rate': 9.940873757549814e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 875/1644 [14:31<09:21,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.202, 'grad_norm': 0.3017258644104004, 'learning_rate': 9.448431908821043e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 900/1644 [14:48<08:19,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 8.957328856066061e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 925/1644 [15:10<08:52,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1348, 'grad_norm': 0.8902373313903809, 'learning_rate': 8.468756631239326e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 950/1644 [15:26<07:44,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 7.983901123332538e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 975/1644 [15:48<08:12,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0816, 'grad_norm': 0.23435111343860626, 'learning_rate': 7.50393919992059e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 1000/1644 [16:04<07:14,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3431.3759, 'grad_norm': 0.0, 'learning_rate': 7.030035850604793e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1025/1644 [16:26<07:39,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0741, 'grad_norm': 0.1483297497034073, 'learning_rate': 6.563341359286943e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 1050/1644 [16:43<06:46,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 829.7062, 'grad_norm': 0.0, 'learning_rate': 6.104988512137877e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 1075/1644 [17:04<07:07,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0847, 'grad_norm': 0.19945167005062103, 'learning_rate': 5.656089848037489e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1100/1644 [17:22<06:12,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4101.5569, 'grad_norm': 0.0, 'learning_rate': 5.217734958160002e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 1125/1644 [17:43<06:15,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1277, 'grad_norm': 0.43563875555992126, 'learning_rate': 4.7909878412592344e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 1150/1644 [18:00<05:31,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 4.3768843210731244e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 1175/1644 [18:21<05:46,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0834, 'grad_norm': 0.18889422714710236, 'learning_rate': 3.9764295321162e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 1200/1644 [18:38<04:58,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3854.6947, 'grad_norm': 0.0, 'learning_rate': 3.5905954799625976e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 1225/1644 [18:59<05:22,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0955, 'grad_norm': 0.14254984259605408, 'learning_rate': 3.220318681941426e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1250/1644 [19:16<04:24,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 453.7752, 'grad_norm': 0.0, 'learning_rate': 2.8664978939711407e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1275/1644 [19:38<04:35,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0537, 'grad_norm': 0.17924343049526215, 'learning_rate': 2.529991929050447e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 1300/1644 [19:55<03:52,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1673.7922, 'grad_norm': 0.0, 'learning_rate': 2.2116175727008458e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 1325/1644 [20:16<03:52,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4224, 'grad_norm': 0.0, 'learning_rate': 1.9121476004205062e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 1350/1644 [20:33<03:19,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.6323089019617045e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▎ | 1375/1644 [20:54<03:22,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1006, 'grad_norm': 0.17611193656921387, 'learning_rate': 1.3727807169845986e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 1400/1644 [21:11<02:48,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1813.9475, 'grad_norm': 0.0, 'learning_rate': 1.134192986369904e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1425/1644 [21:33<02:43,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.153, 'grad_norm': 0.3075175881385803, 'learning_rate': 9.17124823192239e-06, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1450/1644 [21:50<02:10,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6054.6019, 'grad_norm': 0.0, 'learning_rate': 7.22103107065466e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 1475/1644 [22:11<02:08,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0781, 'grad_norm': 0.18055270612239838, 'learning_rate': 5.496012052719335e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1500/1644 [22:32<02:40,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1860.9647, 'grad_norm': 0.0, 'learning_rate': 4.000378237797453e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 1525/1644 [23:02<02:23,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1706, 'grad_norm': 0.38105010986328125, 'learning_rate': 2.737759909369353e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1550/1644 [23:31<01:47,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 1.711221763093751e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1575/1644 [24:00<01:21,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.081, 'grad_norm': 0.22118328511714935, 'learning_rate': 9.232554680123517e-07, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 1600/1644 [24:29<00:48,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2208.2502, 'grad_norm': 0.0, 'learning_rate': 3.757736186355998e-07, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1625/1644 [24:54<00:13,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8377, 'grad_norm': 0.0, 'learning_rate': 7.010509258965625e-08, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1644/1644 [25:19<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1519.5761, 'train_samples_per_second': 17.309, 'train_steps_per_second': 1.082, 'train_loss': 1251.7009366678205, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1644, training_loss=1251.7009366678205, metrics={'train_runtime': 1519.5761, 'train_samples_per_second': 17.309, 'train_steps_per_second': 1.082, 'total_flos': 3.202828094133043e+16, 'train_loss': 1251.7009366678205, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('text', data_files=\"formatted_1000_stories.txt\", split=\"train\")\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "  major,_= torch.cuda.get_device_capability()\n",
    "  if major >= 8:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps= gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set Supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "# Training model\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the train Model and running it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] a story from villain's point of view [/INST] The world was mine for the taking. Or so I thought. As the infamous Dr. Destructo, I had been terrorizing the city for months, leaving a trail of destruction in my wake. But this time, I had a new plan. A plan to finally take my rightful place as the supreme ruler of the world. And nothing would stop me. Not even the heroes who kept trying to stop me. I was Dr. Destructo, hear me roar! Or at least, I would be. Once I had completed my plan. But first, I had to get my hands on the one thing that would make it all possible: the Omega Orb. A powerful artifact that would give me the ultimate power to reshape the world in my image. But it was guarded by the heroes who had thwarted me at every turn. I would have to get past them if I wanted to achieve my dream. And I would stop at nothing to do it. Nothing. Because this was my destiny. My legacy. My life's work. And I would not be denied. Not by a bunch of self-righteous, self-satisfied, sanctimonious, holier-than-thou, condescending, patronizing, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug, smug,\n"
     ]
    }
   ],
   "source": [
    "prompt = \"a story from villain's point of view\"\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=600, truncation=True)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\", eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Optional: Check and correct if the story ends mid-sentence.\n",
    "# generated_text = result[0]['generated_text']\n",
    "# if not generated_text.endswith('.'):\n",
    "#     generated_text = generated_text.rstrip('.').rsplit('.', 1)[0] + '.'\n",
    "\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run test generation pipeline with model\n",
    "# prompt = \"A dog who is very loyal to its owner and they get lost in foreset\"\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=600, no_repeat_ngram_size=3, early_stopping=True)\n",
    "# result = pipe(f\"<s>[INST] {prompt} [/INST]\", eos_token_id=tokenizer.eos_token_id)\n",
    "# print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] a story from villain's point of view. [/INST] The villain sat in his dark, dank lair, brooding over the latest defeat. Another day, another city reduced to rubble by his unstoppable powers. He couldn't help but feel a twinge of disappointment. The heroes always seemed to be one step ahead of him, always ready to thwart his plans. But he refused to give up. He would find a way to outsmart them, to finally achieve his ultimate goal of world domination. He just needed to be patient, to keep plotting and scheming. The day of reckoning would come. And when it did, the heroes would be powerless to stop him. The villain's reign would begin. With a scowl, he turned his attention to the next plan, the next city to be destroyed. It was all just a matter of time. The world would soon tremble before his power. And the heroes would beg for mercy on their knees. But the villain would not show them any. He would rule with an iron fist, crushing all opposition. And when he was done, the world would be his to command. The villain's reign had begun. The end. [/INST] The villain sat in his dark, dank lair, brooding over the latest defeat. Another day, another city reduced to rubble by his unstoppable powers. He couldn't help but feel a twinge of disappointment. The heroes always seemed to be one step ahead of him, always ready to thwart his plans. But he refused to give up. He would find a way to outsmart them, to finally achieve his ultimate goal of world domination. He just needed to be patient, to keep plotting and scheming. The day of reckoning would come. And when it did, the heroes would be powerless to stop him. The villain's reign would begin. With a scowl, he turned his attention to the next plan, the next city to be destroyed. It was all just a matter of time. The world would soon tremble before his power. And the heroes would beg for mercy on their knees. But the villain would not show them any. He would rule with an iron fist, crushing all opposition. And when he was done, the world would be his to command. The villain's reign had begun. The end. [/INST] The villain sat in his dark, dank lair, brooding over the latest defeat. Another day, another city reduced to rubble by his unstoppable powers. He couldn't help but feel a twinge of disappointment. The heroes always seemed to be one step\n"
     ]
    }
   ],
   "source": [
    "# Run test generation pipeline with model\n",
    "# A story that you have been reading for seven years suddenly becames your reality and you are the only one who knows how this story ends. You are determined to change the end of the world even if it means to become the only companion of the main protaginist\n",
    "prompt = \"a story from villain's point of view.\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=600, truncation=True)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model into hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.03s/it]\n",
      "c:\\Users\\admin\\Documents\\loom\\.venv\\Lib\\site-packages\\peft\\peft_model.py:556: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  adapters_weights = torch.load(\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Push the model into HF 😀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\admin\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shrujan142/LoomAI/commit/4c6c5dcb00923b63bd39bc359d03b0dd767c67c8', commit_message='Upload LlamaForCausalLM', commit_description='', oid='4c6c5dcb00923b63bd39bc359d03b0dd767c67c8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"shrujan142/LoomAI\", check_pr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/shrujan142/LoomAI/commit/4c6c5dcb00923b63bd39bc359d03b0dd767c67c8', commit_message='Upload tokenizer', commit_description='', oid='4c6c5dcb00923b63bd39bc359d03b0dd767c67c8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer.push_to_hub(\"shrujan142/LoomAI\",check_pr=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
